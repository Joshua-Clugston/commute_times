{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b3043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commute = pd.read_csv('../data/census_data/2021_Data.csv')\n",
    "commute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba74335",
   "metadata": {},
   "source": [
    "Let's get rid of those pesky Error and Percent Allocated columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern  = re.compile('Margin of Error')\n",
    "pattern2 = re.compile('PERCENT ALLOCATED')\n",
    "drop = []\n",
    "\n",
    "for col in commute.columns:\n",
    "    if re.search(pattern,col):\n",
    "        #print(col)\n",
    "        drop.append(col)\n",
    "    elif re.search(pattern2,col):\n",
    "        #print(col)\n",
    "        drop.append(col)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4baf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "commute = commute.drop(columns = drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8de67e",
   "metadata": {},
   "source": [
    "And the random Unnamed column while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef21a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commute = commute.drop(columns = 'Unnamed: 778')\n",
    "#commute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e944b",
   "metadata": {},
   "source": [
    "Having the letter \"N\" doesn't help... let's replace it with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff958aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commute = commute.replace(to_replace='N',value=np.nan)\n",
    "#commute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a097592",
   "metadata": {},
   "source": [
    "And let's clean up those columns names! There's a lot going on with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "\n",
    "for col in commute.columns:\n",
    "    new_col = col.replace('Estimate!!Total!!', '').replace('!!',' ').strip()\n",
    "    new_cols.append(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75206b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commute.columns = new_cols\n",
    "#commute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca5df5",
   "metadata": {},
   "source": [
    "Pull the columns strictly related to commute times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('TRAVEL TIME')\n",
    "drop = []\n",
    "\n",
    "for col in commute.columns:\n",
    "    if (not re.search(pattern,col)) and ((col != 'Geographic Area Name') and (col != 'Workers 16 years and over')\n",
    "                                        and (col != 'Workers 16 years and over who did not work from home') # These columns needed to stay\n",
    "                                        and (col != 'Estimate Car, truck, or van -- drove alone Workers 16 years and over')\n",
    "                                        and (col != 'Estimate Car, truck, or van -- carpooled Workers 16 years and over')\n",
    "                                        and (col != 'Estimate Public transportation (excluding taxicab) Workers 16 years and over')):\n",
    "        drop.append(col)\n",
    "\n",
    "commutes = commute.drop(columns = drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deebde4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3d915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes = commutes.set_index('Geographic Area Name')\n",
    "#commutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6592eb",
   "metadata": {},
   "source": [
    "Checking my home county just to make sure it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b20db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes.loc['Montgomery County, Maryland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31a74f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee3a74",
   "metadata": {},
   "source": [
    "## The data types are wrong. This will have to be something that will be cleaned up.\n",
    "\n",
    "And we can't groupby anything because apparently there are '-'  floating around..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ce9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes = commutes.replace(to_replace = '-', value = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6bc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes = commutes.apply(pd.to_numeric)\n",
    "commutes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc982e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53605a",
   "metadata": {},
   "source": [
    "With that done, let's make a 'State' column (we'll group by it later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "\n",
    "for ind, values in commutes.iterrows():\n",
    "    state = ind.split(', ')[1]\n",
    "    #print(state)\n",
    "    states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade9273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes['State'] = states\n",
    "#commutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a09084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes.groupby('State').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37263ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', None)\n",
    "#commutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6e48d",
   "metadata": {},
   "source": [
    "There are a lot of counties that have mostly null values... Let's drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a5fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_nan = commutes.dropna(thresh = 30)\n",
    "commutes_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556aaa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_nan.groupby('State').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa7422",
   "metadata": {},
   "source": [
    "I need to be careful with how I approach this data now. Taking averages of percents can be useful (I literally did a mathematical proof that shows that taking averages of percentages will still yield a row that sums to 100%), but if each sample size is different, percentages will become skewed and meaningless.\n",
    "\n",
    "Averages are fine for the mean travel times, but other than that, they should be avoided. I would like to get rid of the percentages by multiplying the total number of people to the percentage, but this dataset doesn't actually give me a breakdown of how many people commute, much less which category they fall under. I can maybe try looking for this info, but I'm not sure I would find it. Perhaps I should start focusing somewhere else.\n",
    "\n",
    "# OF COURSE THE DATA WAS ALREADY IN THE DATASET. I had simply dropped it earlier. *Now revised: See Cell 10*\n",
    "\n",
    "Well, this should be fun then.\n",
    "\n",
    "First, I need to find a way to get rid of the percentages and put in the actual number of people. This way, I can take averages without skewing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in commutes_nan.itertuples():\n",
    "    #print('THE ROW INDEX IS:',row.Index)\n",
    "    \n",
    "    comm_total  = row[2]\n",
    "    drove_alone = row[13]\n",
    "    carpool     = row[24]\n",
    "    pub_transit = row[35]\n",
    "\n",
    "    for i in range(3,12):\n",
    "        #print('\\n',i)\n",
    "        try:\n",
    "            new_val = int(comm_total*(row[i]/100))\n",
    "            #print(f'{comm_total} times {row[i]/100} equals {new_val}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = new_val\n",
    "        except:\n",
    "            #print(f'{comm_total} times {row[i]/100} equals {np.nan}\\t{commutes.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = np.nan\n",
    "    \n",
    "    \n",
    "    for i in range(14,23):\n",
    "        #print('\\n',i)\n",
    "        try:\n",
    "            new_val = int(drove_alone*(row[i]/100))\n",
    "            #print(f'{drove_alone} times {row[i]/100} equals {new_val}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = new_val\n",
    "        except:\n",
    "            #print(f'{drove_alone} times {row[i]/100} equals {np.nan}\\t{commutes_nnan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = np.nan\n",
    "    \n",
    "    \n",
    "    for i in range(25,34):\n",
    "        #print('\\n',i)\n",
    "        try:\n",
    "            new_val = int(carpool*(row[i]/100))\n",
    "            #print(f'{carpool} times {row[i]/100} equals {new_val}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = new_val\n",
    "        except:\n",
    "            #print(f'{carpool} times {row[i]/100} equals {np.nan}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = np.nan\n",
    "            \n",
    "            \n",
    "    for i in range(36,45):\n",
    "        #print('\\n',i)\n",
    "        try:\n",
    "            new_val = int(pub_transit*(row[i]/100))\n",
    "            #print(f'{pub_transit} times {row[i]/100} equals {new_val}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = new_val\n",
    "        except:\n",
    "            #print(f'{pub_transit} times {row[i]/100} equals {np.nan}\\t{commutes_nan.columns[i-1]}')\n",
    "            commutes_nan.loc[row.Index, commutes_nan.columns[i-1]] = np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76c5c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#commutes_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2468262",
   "metadata": {},
   "source": [
    "Since a lot of these rows are null, I'm gonna drop any that are mostly null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_nan = commutes.dropna(thresh = 30)\n",
    "#commutes_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7a527",
   "metadata": {},
   "source": [
    "Now, let's group by State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e232cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_avg = commutes_nan.groupby('State').mean().astype(int)\n",
    "commutes_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b6b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_avg.sort_values('Workers 16 years and over who did not work from home TRAVEL TIME TO WORK Mean travel time to work (minutes)', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bcf78",
   "metadata": {},
   "source": [
    "Cleaning up the columns names... again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28878e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r'Workers 16 years and over who did not work from home'\n",
    "pattern2 = r'Estimate'\n",
    "pattern3 = r'(excluding taxicab)'\n",
    "\n",
    "new_cols = {}\n",
    "\n",
    "for col in commutes_avg.columns:\n",
    "    if (col != 'Workers 16 years and over who did not work from home') and (re.search(pattern1,col) \n",
    "                                                                            or re.search(pattern2,col) \n",
    "                                                                            or re.search(pattern3,col)):\n",
    "        new_col = col.replace(str(pattern1),'').replace(str(pattern2),'').replace(str(pattern3),'').strip()\n",
    "        new_cols[col] = new_col\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e5e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_avg = commutes_avg.rename(columns = new_cols)\n",
    "commutes_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c10c7",
   "metadata": {},
   "source": [
    "My goal is to now do this process for every dataset I have access to... which would be easy with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_commute(path):\n",
    "    ### read in data\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    \n",
    "    ### create name of file for saving\n",
    "    num = path.split('_data/')[1].split('.csv')[0]\n",
    "    file = num + '_Commute'\n",
    "    \n",
    "    ### get rid of specific columns\n",
    "    #print('Get rid of specific columns')\n",
    "    pattern1 = r'Margin of Error'\n",
    "    pattern2 = r'PERCENT ALLOCATED'\n",
    "    pattern3 = r'Unnamed:'\n",
    "\n",
    "    drop = []\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if (re.search(pattern1,col) or re.search(pattern2,col) or re.search(pattern3,col)):\n",
    "            drop.append(col)\n",
    "    #end\n",
    "    \n",
    "    data = data.drop(columns = drop)\n",
    "    \n",
    "    ### clean up columns names\n",
    "    #print('Clean up columns names')\n",
    "    new_cols = []\n",
    "\n",
    "    for col in data.columns:\n",
    "        new_col = col.replace('Estimate','').replace('Total', '').replace('!!',' ').replace('  ',' ').strip()\n",
    "        new_cols.append(new_col)\n",
    "    #end\n",
    "    \n",
    "    data.columns = new_cols\n",
    "    \n",
    "    \n",
    "    ### remove rows that are mostly null or where 'Workers 16 years and over who did not work from home' is null\n",
    "    ### ALSO CHECK IF THIS COLUMN EXISTS!\n",
    "    #print('Determine j')\n",
    "    data = data.dropna(thresh = 30)\n",
    "    \n",
    "    if 'Workers 16 years and over who did not work from home' in data.columns:\n",
    "        j = 1 # this indicates that my original code will work with this data\n",
    "        print('j is',j)\n",
    "        \n",
    "        drop = []\n",
    "        for ind, values in data.iterrows():\n",
    "            if pd.isna(values['Workers 16 years and over who did not work from home']):\n",
    "                drop.append(ind)\n",
    "    else:\n",
    "        j = 0\n",
    "        print('j is',j)\n",
    "        drop = []\n",
    "        \n",
    "        for ind, values in data.iterrows():\n",
    "            if pd.isna(values['Workers 16 years and over']):\n",
    "                drop.append(ind)\n",
    "    #end\n",
    "    \n",
    "    data = data.drop(index = drop)    \n",
    "    \n",
    "    \n",
    "    ### keep specific commute related columns\n",
    "    #print('Keep specific commute related columns')\n",
    "    if j == 0:\n",
    "        pattern1 = r'TRAVEL TIME'\n",
    "    else:\n",
    "        pattern1 = r'TRAVEL TIME'\n",
    "    \n",
    "    if j == 0:\n",
    "        keep = ['Geographic Area Name','Workers 16 years and over','Car, truck, or van -- drove alone Workers 16 years and over',\n",
    "                'Car, truck, or van -- carpooled Workers 16 years and over','Public transportation (excluding taxicab) Workers 16 years and over']\n",
    "    else:\n",
    "        keep = ['Geographic Area Name','Workers 16 years and over','Workers 16 years and over who did not work from home',\n",
    "                'Car, truck, or van -- drove alone Workers 16 years and over','Car, truck, or van -- carpooled Workers 16 years and over',\n",
    "                'Public transportation (excluding taxicab) Workers 16 years and over']\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if re.search(pattern1,col):\n",
    "            keep.append(col)\n",
    "    #end\n",
    "    \n",
    "    data = data[keep]\n",
    "\n",
    "    \n",
    "    ### Set county as index\n",
    "    #print('Set county as index')\n",
    "\n",
    "    data = data.set_index('Geographic Area Name')\n",
    "    \n",
    "    \n",
    "    ### replace N and - with NaN\n",
    "    #print('Replace N and - with NaN')\n",
    "    data = data.replace(to_replace='N',value=np.nan).replace(to_replace = '-', value = np.nan).replace('**',np.nan)\n",
    "    \n",
    "    data = data.dropna(thresh = 25)\n",
    "    \n",
    "    \n",
    "    ### make dtypes numeric\n",
    "    #print('Make df numeric')\n",
    "    data = data.apply(pd.to_numeric)\n",
    "    \n",
    "    \n",
    "    ### get rid of percentages by translating them to integers BASED ON j\n",
    "    if j == 0:\n",
    "        for row in data.itertuples():\n",
    "            #print('\\tROW IS NOW:\\t',row.Index)\n",
    "            comm_total  = row[1]\n",
    "            drove_alone = row[2]\n",
    "            carpool     = row[3]\n",
    "            pub_transit = row[4]\n",
    "\n",
    "            for i in range(5,41):\n",
    "                #print('i is',i)\n",
    "                if i%4 == 1: # total\n",
    "                    try:\n",
    "                        #print('--> COMM_TOTAL\\nCOMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                        new_val = int(comm_total*(row[i]/100))\n",
    "                        #print('CHECK:',comm_total,'times',row[i]/100,'equals',new_val)\n",
    "                        \n",
    "                        data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                    except:\n",
    "                        data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "                    #end\n",
    "                if i%4 == 2: # drove alone\n",
    "                    try:\n",
    "                        #print('--> DROVE_ALONE\\nCOMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                        new_val = int(drove_alone*(row[i]/100))\n",
    "                        #print('CHECK:',drove_alone,'times',row[i]/100,'equals',new_val)\n",
    "                        \n",
    "                        data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                    except:\n",
    "                        data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "                    #end\n",
    "                if i%4 == 3: # carpool\n",
    "                    try:\n",
    "                        #print('--> CARPOOL\\nCOMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                        new_val = int(carpool*(row[i]/100))\n",
    "                        #print('CHECK:',carpool,'times',row[i]/100,'equals',new_val)\n",
    "                        \n",
    "                        data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                    except:\n",
    "                        data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "                    #end\n",
    "                if i%4 == 0: # public transit\n",
    "                    try:\n",
    "                        #print('--> PUB_TRANSIT\\nCOMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                        new_val = int(pub_transit*(row[i]/100))\n",
    "                        #print('CHECK:',pub_transit,'times',row[i]/100,'equals',new_val)\n",
    "                        \n",
    "                        data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                    except:\n",
    "                        data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "                    #end\n",
    "            #end\n",
    "    elif j == 1:\n",
    "        for row in data.itertuples():\n",
    "            #print('ROW IS:',row.Index)\n",
    "            comm_total  = row[2]\n",
    "            drove_alone = row[3]\n",
    "            carpool     = row[4]\n",
    "            pub_transit = row[5]\n",
    "\n",
    "            for i in range(6,15):\n",
    "                #print('i is',i)\n",
    "                try:\n",
    "                    #print('COMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                    new_val = int(comm_total*(row[i]/100))\n",
    "                    #print('CHECK:',comm_total,'times',row[i]/100,'equals',new_val)\n",
    "                    \n",
    "                    data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                except:\n",
    "                    data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "\n",
    "\n",
    "            for i in range(16,25):\n",
    "                #print('i is',i)\n",
    "                try:\n",
    "                    #print('COMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                    new_val = int(drove_alone*(row[i]/100))\n",
    "                    #print('CHECK:',drove_alone,'times',row[i]/100,'equals',new_val)\n",
    "                    \n",
    "                    data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                except:\n",
    "                    data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "\n",
    "\n",
    "            for i in range(26,35):\n",
    "                #print('i is',i)\n",
    "                try:\n",
    "                    #print('COMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                    new_val = int(carpool*(row[i]/100))\n",
    "                    #print('CHECK:',carpool,'times',row[i]/100,'equals',new_val)\n",
    "                    \n",
    "                    data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                except:\n",
    "                    data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "\n",
    "\n",
    "            for i in range(36,45):\n",
    "                #print('i is',i)\n",
    "                try:\n",
    "                    #print('COMPARE:',row[i],'to',data.loc[row.Index, data.columns[i-1]])\n",
    "                    new_val = int(pub_transit*(row[i]/100))\n",
    "                    #print('CHECK:',pub_transit,'times',row[i]/100,'equals',new_val)\n",
    "                    \n",
    "                    data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "                except:\n",
    "                    data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "        #end\n",
    "    \n",
    "    \n",
    "    ### Clean up column names some more\n",
    "    pattern1 = r'Workers 16 years and over who did not work from home'\n",
    "    pattern2 = r'Estimate'\n",
    "    pattern3 = r'(excluding taxicab)'\n",
    "\n",
    "    new_cols = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        if (col != 'Workers 16 years and over who did not work from home') and (re.search(pattern1,col) \n",
    "                                                                                or re.search(pattern2,col) \n",
    "                                                                                or re.search(pattern3,col)):\n",
    "            new_col = col.replace(str(pattern1),'').replace(str(pattern2),'').replace(str(pattern3),'').strip()\n",
    "            new_cols[col] = new_col\n",
    "    #end\n",
    "    \n",
    "    data = data.rename(columns = new_cols)\n",
    "    \n",
    "    \n",
    "    ### save county file\n",
    "    data.to_csv(f'../data/cleaned_data/{file}_County_Sum.csv')\n",
    "    \n",
    "    \n",
    "    ### set up aggregations for group by\n",
    "    aggs = {}\n",
    "    \n",
    "    for col in data.columns:        \n",
    "        num = data.columns.get_loc(col)\n",
    "        \n",
    "        if j == 0:\n",
    "            if num < 39:\n",
    "                aggs[col] = np.sum\n",
    "            else:\n",
    "                aggs[col] = np.mean\n",
    "            #end\n",
    "        elif j == 1:\n",
    "            if num in [14,24,34,44]:\n",
    "                aggs[col] = np.mean\n",
    "            else:\n",
    "                aggs[col] = np.sum\n",
    "    #end\n",
    "    \n",
    "    \n",
    "    ### add State column (so we can group by it)\n",
    "    states = []\n",
    "\n",
    "    for ind, values in data.iterrows():\n",
    "        state = ind.split(', ')[1]\n",
    "        states.append(state)\n",
    "    #end\n",
    "    \n",
    "    data['State'] = states\n",
    "    \n",
    "    \n",
    "    ### finally, group by state and return df\n",
    "    \n",
    "    data = data.groupby('State').agg(aggs).replace(np.nan, 0).astype(int)#.replace(to_replace=0,value=np.nan)\n",
    "    \n",
    "    data.to_csv(f'../data/cleaned_data/{file}_State_Sum.csv')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58793484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = avg_commute('../data/census_data/2010_Data.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e49f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bf907",
   "metadata": {},
   "source": [
    "The first few times I ran this, the number of people driving alone would be larger than the total of people who didn't work from home... which doesn't make any sense. I had to investigate for a bit, but I found out that there were several rows that had NaN for total workers not working at home but some number for driving alone. I decided to remove these columns to make the data as uniform as possible and have as few errors as possible (even though some data is now useless, but I believe it's worth it).\n",
    "\n",
    "I first checked to see if using a list instead of an index would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(commutes_avg.columns[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaca05",
   "metadata": {},
   "source": [
    "But it did not.\n",
    "\n",
    "I then checked to see where total workers (including at home workers) was null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes.loc[pd.isna(commutes['Workers 16 years and over'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3dd7",
   "metadata": {},
   "source": [
    "But silly me, that's not the column I was interested in nor the table I needed to check! I think this is why I was going in circles for a bit.\n",
    "\n",
    "I decided to see if there was any row where this contradiciton happened (more drivers than workers travelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes_avg.loc[commutes_avg['Car, truck, or van -- drove alone Workers 16 years and over']\n",
    "            > commutes_avg['Workers 16 years and over who did not work from home']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e335ab2",
   "metadata": {},
   "source": [
    "Nothing... that's wasn't very helpful.\n",
    "\n",
    "Since I had come up with the separate aggregations for grouping by within the function, I tried it out here to see if anything changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {}\n",
    "\n",
    "for col in commutes_nan.columns:\n",
    "    num = commutes_nan.columns.get_loc(col)\n",
    "    if (num < 2 or num == 12 or num == 23 or num == 34):\n",
    "        aggs[col] = np.sum\n",
    "    elif num < 45:\n",
    "        aggs[col] = np.mean\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba496b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_agg = commutes_nan.groupby('State').agg(aggs)\n",
    "#commutes_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a3d74",
   "metadata": {},
   "source": [
    "But nothing changed.\n",
    "\n",
    "I then checked the df I brought in with the function to see if *that* did anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['Estimate Car, truck, or van -- drove alone Workers 16 years and over']\n",
    "#            > df['Workers 16 years and over who did not work from home']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f29b38",
   "metadata": {},
   "source": [
    "And of course it didn't.\n",
    "\n",
    "Finally, I decided to replace all NaN's with 0's and then compare. Maybe this would get me somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f268c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_zero = commutes_nan.replace(to_replace=np.nan, value = 0)\n",
    "#commutes_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732b693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commutes_zero.loc[commutes_zero['Estimate Car, truck, or van -- drove alone Workers 16 years and over']\n",
    "            > commutes_zero['Workers 16 years and over who did not work from home']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc44ca1",
   "metadata": {},
   "source": [
    "#### *AHA!* There's the problem!\n",
    "\n",
    "Many of these rows didn't have anything listed for workers who had to travel, but whenever I would try comparing them numerically, nothing showed up because they were NaN: 'Not a Number'\n",
    "\n",
    "So with this in mind, I went to remove any row where this column was null. That was able to fix it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00b636",
   "metadata": {},
   "source": [
    "The last step for this notebook is to clean each table and save it so we can pull it into a fresh new notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b24e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range (2010,2023):\n",
    "    if i != 2020:\n",
    "        print(i)\n",
    "        \n",
    "        try:\n",
    "            path = f'../data/census_data/{i}_Data.csv'\n",
    "            new_df = avg_commute(path)\n",
    "            #new_df.to_csv(f'../data/cleaned_data/{i}_Data_State.csv')\n",
    "            print('Completed')\n",
    "        except:\n",
    "            print('Skipped')\n",
    "            continue\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db70a0",
   "metadata": {},
   "source": [
    "Some things went wrong... It's time to take a closer look at each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d37ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = avg_commute('../data/census_data/2010_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d609f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/census_data/2010_Data.csv')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69df46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### get rid of specific columns\n",
    "pattern1 = r'Margin of Error'\n",
    "pattern2 = r'PERCENT ALLOCATED'\n",
    "pattern3 = r'Unnamed:'\n",
    "\n",
    "drop = []\n",
    "  \n",
    "for col in data.columns:\n",
    "    if (re.search(pattern1,col) or re.search(pattern2,col) or re.search(pattern3,col)):\n",
    "        drop.append(col)\n",
    "#end\n",
    "    \n",
    "data = data.drop(columns = drop)\n",
    " \n",
    "### clean up columns names\n",
    "new_cols = []\n",
    "\n",
    "for col in data.columns:\n",
    "    new_col = col.replace('Estimate', '').replace('Total','').replace('!!',' ').replace('  ',' ').strip()\n",
    "    new_cols.append(new_col)\n",
    "#end\n",
    "    \n",
    "data.columns = new_cols\n",
    "\n",
    "\n",
    "### keep specific commute related columns\n",
    "pattern = r'TRAVEL TIME'\n",
    "drop = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if (not re.search(pattern,col)\n",
    "        and ((col != 'Geographic Area Name') \n",
    "        and (col != 'Workers 16 years and over')\n",
    "        and (col != 'Workers 16 years and over who did not work from home')\n",
    "        and (col != 'Car, truck, or van -- drove alone Workers 16 years and over')\n",
    "        and (col != 'Car, truck, or van -- carpooled Workers 16 years and over')\n",
    "        and (col != 'Public transportation (excluding taxicab) Workers 16 years and over'))):\n",
    "        drop.append(col)\n",
    "#end\n",
    "\n",
    "data = data.drop(columns = drop)\n",
    "\n",
    "\n",
    "### Set county as index\n",
    "data = data.set_index('Geographic Area Name')\n",
    "\n",
    "    \n",
    "### replace N and - with NaN\n",
    "data = data.replace(to_replace='N',value=np.nan).replace(to_replace = '-', value = np.nan).replace('**',np.nan)\n",
    "    \n",
    "    \n",
    "### remove rows that are mostly null or where 'Workers 16 years and over who did not work from home' is null\n",
    "data = data.dropna(thresh = 30)\n",
    "\n",
    "drop = []\n",
    "if 'Workers 16 years and over who did not work from home' in data.columns:\n",
    "    j = 1 # this indicates that my original code will work with this data\n",
    "        \n",
    "    for ind, values in data.iterrows():\n",
    "        if pd.isna(values['Workers 16 years and over who did not work from home']):\n",
    "            drop.append(ind)\n",
    "            \n",
    "elif 'Workers 16 years and over who did not work at home' in data.columns:\n",
    "    j = 2 # this indicates that my original code will *almost* work with this data\n",
    "        \n",
    "    for ind, values in data.iterrows():\n",
    "        if pd.isna(values['Workers 16 years and over who did not work at home']):\n",
    "            drop.append(ind)\n",
    "else:\n",
    "    j = 0\n",
    "#end\n",
    "    \n",
    "data = data.drop(index = drop)\n",
    "    \n",
    "    \n",
    "### make dtypes numeric\n",
    "data = data.apply(pd.to_numeric)\n",
    "\n",
    "print('j is',j)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d0f9a",
   "metadata": {},
   "source": [
    "**Interesting Note:** this dataset does not have workers *who did not work from home* listed. I guess that makes sense because remote work was not nearly as common then as it is now (particularly because of the pandemic). Because of this difference, I'll have to adjust the function to account for this missing column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26155972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401762ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data.itertuples():\n",
    "    comm_total  = row[1]\n",
    "    drove_alone = row[2]\n",
    "    carpool     = row[3]\n",
    "    pub_transit = row[4]\n",
    "    \n",
    "    for i in range(5,41):\n",
    "        if i%4 == 1: # total\n",
    "            try:\n",
    "                new_val = int(comm_total*(row[i]/100))\n",
    "                data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "            except:\n",
    "                data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "            #end\n",
    "        if i%4 == 2: # drove alone\n",
    "            try:\n",
    "                new_val = int(drove_alone*(row[i]/100))\n",
    "                data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "            except:\n",
    "                data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "            #end\n",
    "        if i%4 == 3: # carpool\n",
    "            try:\n",
    "                new_val = int(carpool*(row[i]/100))\n",
    "                data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "            except:\n",
    "                data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "            #end\n",
    "        if i%4 == 0: # public transit\n",
    "            try:\n",
    "                new_val = int(pub_transit*(row[i]/100))\n",
    "                data.loc[row.Index, data.columns[i-1]] = new_val\n",
    "            except:\n",
    "                data.loc[row.Index, data.columns[i-1]] = np.nan\n",
    "            #end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94440c",
   "metadata": {},
   "source": [
    "This code helped for the most part, but 2015 and 2018 are being difficult.\n",
    "\n",
    "2015's table name was incorrect! At least it was an easy fix.\n",
    "\n",
    "2018 on the other hand... let's read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c6a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/census_data/2018_Data.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078134b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_csv = avg_commute('../data/census_data/2018_Data.csv')\n",
    "data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec94d64",
   "metadata": {},
   "source": [
    "For some reason, that fixed it. Not sure why, but it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc59c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
